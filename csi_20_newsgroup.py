# -*- coding: utf-8 -*-
"""CSI_20_NewsGroup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1smBToecD33B3Frc84wUqD20l_YfsFUUK
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.manifold import TSNE
from sklearn import metrics
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from time import time
from collections import defaultdict
from wordcloud import WordCloud

# Download NLTK resources
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

# Load all 20 newsgroups categories
categories = [
    'alt.atheism',
    'comp.graphics',
    'comp.os.ms-windows.misc',
    'comp.sys.ibm.pc.hardware',
    'comp.sys.mac.hardware',
    'comp.windows.x',
    'misc.forsale',
    'rec.autos',
    'rec.motorcycles',
    'rec.sport.baseball',
    'rec.sport.hockey',
    'sci.crypt',
    'sci.electronics',
    'sci.med',
    'sci.space',
    'soc.religion.christian',
    'talk.politics.guns',
    'talk.politics.mideast',
    'talk.politics.misc',
    'talk.religion.misc'
]

print("Loading 20 newsgroups dataset for all categories...")
dataset = fetch_20newsgroups(subset='all', categories=categories,
                           shuffle=True, random_state=42,
                           remove=('headers', 'footers', 'quotes'))
true_k = len(categories)
print(f"Number of categories: {true_k}")

sample_size = 5000
if len(dataset.data) > sample_size:
    print(f"Sampling {sample_size} documents for faster processing...")
    indices = np.random.choice(len(dataset.data), sample_size, replace=False)
    dataset.data = [dataset.data[i] for i in indices]
    dataset.target = dataset.target[indices]

# Perform Lemmatization
print("Performing lemmatization...")
lemmatizer = WordNetLemmatizer()
lemmatized_data = []
for doc in dataset.data:
    word_list = word_tokenize(doc)
    lemmatized_doc = " ".join([lemmatizer.lemmatize(word) for word in word_list])
    lemmatized_data.append(lemmatized_doc)
dataset.data = lemmatized_data

# TF-IDF Vectorization for K-means
print("Creating TF-IDF vectors...")
tfidf_vectorizer = TfidfVectorizer(strip_accents='unicode',
                                 stop_words='english',
                                 min_df=5, max_df=0.5,
                                 sublinear_tf=True)
X_tfidf = tfidf_vectorizer.fit_transform(dataset.data)
print(f"TF-IDF matrix shape: {X_tfidf.shape}")

# Count Vectorization for LDA
print("Creating Count vectors for LDA...")
count_vectorizer = CountVectorizer(strip_accents='unicode',
                                 stop_words='english',
                                 min_df=5, max_df=0.5)
X_counts = count_vectorizer.fit_transform(dataset.data)
print(f"Count matrix shape: {X_counts.shape}")

# K-means Clustering
print("\nPerforming K-means clustering...")
km = MiniBatchKMeans(n_clusters=true_k, init='k-means++',
                    batch_size=1000, max_iter=100, n_init=3)
t0 = time()
km.fit(X_tfidf)
print(f"K-means clustering done in {time() - t0:.2f}s")

# LDA Topic Modeling
print("\nPerforming LDA topic modeling...")
# Reduce vocabulary LDA to speed up
max_features = 2000
small_count_vectorizer = CountVectorizer(strip_accents='unicode',
                                       stop_words='english',
                                       max_features=max_features)
X_small_counts = small_count_vectorizer.fit_transform(dataset.data)

lda = LatentDirichletAllocation(n_components=true_k,
                               max_iter=20,
                               learning_method='online',
                               batch_size=128,
                               learning_offset=10.,
                               random_state=42,
                               n_jobs=-1)
t0 = time()
lda.fit(X_small_counts)
print(f"LDA done in {time() - t0:.2f}s")

# Assign LDA topics to documents
lda_topics = lda.transform(X_small_counts).argmax(axis=1)

def evaluate_clustering(true_labels, pred_labels):
    print("\nClustering Evaluation:")
    print(f"Homogeneity: {metrics.homogeneity_score(true_labels, pred_labels):.3f}")
    print(f"Completeness: {metrics.completeness_score(true_labels, pred_labels):.3f}")
    print(f"V-measure: {metrics.v_measure_score(true_labels, pred_labels):.3f}")
    print(f"Adjusted Rand-Index: {metrics.adjusted_rand_score(true_labels, pred_labels):.3f}")

print("\nK-means Evaluation:")
evaluate_clustering(dataset.target, km.labels_)

print("\nLDA Topic Evaluation:")
evaluate_clustering(dataset.target, lda_topics)

# Visualization
def plot_clusters(X_embedded, labels, title, color_map=None):
    plt.figure(figsize=(12, 8))
    if color_map:

        scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1],
                              c=labels, alpha=0.6, cmap='tab20')
        # Create a legend with category names using the color_map and unique numerical labels
        handles = []
        legend_labels = []
        unique_labels = sorted(np.unique(labels))
        for label in unique_labels:
            handle = plt.scatter([], [], color=plt.cm.tab20(label), label=color_map[label])
            handles.append(handle)
            legend_labels.append(color_map[label])
        plt.legend(handles, legend_labels, title="Categories")
    else:
        plt.scatter(X_embedded[:, 0], X_embedded[:, 1],
               c=labels, alpha=0.6, cmap='tab20')
    plt.title(title)
    plt.colorbar()
    plt.show()

tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_tfidf.toarray())

# Plot K-means clusters
plot_clusters(X_tsne, km.labels_, "K-means Clusters")

# Plot LDA topics
plot_clusters(X_tsne, lda_topics, "LDA Topics")

# Plot true categories
category_map = {i: cat for i, cat in enumerate(categories)}
plot_clusters(X_tsne, dataset.target, "True Categories", category_map)

def print_topics(model, feature_names, n_top_words=10):
    for topic_idx, topic in enumerate(model.components_):
        message = f"Topic #{topic_idx}: "
        message += " ".join([feature_names[i]
                           for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(message)

print("LDA Topics:")
print_topics(lda, small_count_vectorizer.get_feature_names_out())
results = pd.DataFrame({
    'Document': dataset.data,
    'True_Category': [categories[i] for i in dataset.target],
    'KMeans_Cluster': km.labels_,
    'LDA_Topic': lda_topics
})
results.to_csv('20newsgroups_clusteringresults.csv', index=False)
print("'20newsgroups_clustering_results.csv'")

def predict_news_category(text):
    # Preprocess text
    words = [word.lower() for word in word_tokenize(text) if word.isalpha()]
    processed_text = " ".join([lemmatizer.lemmatize(word) for word in words])

    # Vectorize
    tfidf_vec = tfidf_vectorizer.transform([processed_text])
    count_vec = small_count_vectorizer.transform([processed_text])

    cluster = km.predict(tfidf_vec)[0]
    distances = km.transform(tfidf_vec)[0]
    nearest_indices = np.argsort(distances)[:3]

    # Get dominan category in cluster
    cluster_mask = results['KMeans_Cluster'] == cluster
    top_categories = results[cluster_mask]['True_Category'].value_counts(normalize=True)
    predicted_category = top_categories.idxmax()
    confidence = top_categories.max()

    # Get LDA keywords
    topic_dist = lda.transform(count_vec)[0]
    top_topic = np.argmax(topic_dist)
    keywords = [small_count_vectorizer.get_feature_names_out()[i]
               for i in lda.components_[top_topic].argsort()[-10:][::-1]]


    similar_docs = [results.iloc[idx]['Document'][:150] + "..."
                  for idx in nearest_indices]

    return {
        'category': predicted_category,
        'confidence': float(confidence),
        'keywords': keywords[:5],
        'cluster': int(cluster),
        'similar_docs': similar_docs
    }

sample = " Analysis of Pauline epistles in early Christianity"
prediction = predict_news_category(sample)

print("Predicted Category:", prediction['category'])
print("Confidence:", f"{prediction['confidence']:.0%}")
print("Keywords:", prediction['keywords'])
print("Cluster:", prediction['cluster'])
print("\nSimilar Documents:")
for i, doc in enumerate(prediction['similar_docs'], 1):
    print(f"{i}. {doc}")